{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df-idf function\n",
    "#the item is the list (ie: productList, desList, searchList)\n",
    "#the name \n",
    "#the output is a dict in a new columns for each product id\n",
    "\n",
    "def df_itf(item,name):\n",
    "    tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df = 0, stop_words = 'english')  #at most gram-2\n",
    "    tfidf_matrix=tf.fit_transform(item)   #item should be pd.Series\n",
    "    feature_names = tf.get_feature_names()\n",
    "    dense = tfidf_matrix.todense()\n",
    "    episode=dense[100].tolist()[0]  # take the row 100 as an example. crush\n",
    "    phrase_scores = [pair for pair in zip(range(0, len(episode)), episode) if pair[1] > 0]\n",
    "    train[name]=0\n",
    "    for i in range(len(dense)):\n",
    "        aadict = {}\n",
    "        episode=dense[i].tolist()[0]\n",
    "        phrase_scores = [pair for pair in zip(range(0, len(episode)), episode) if pair[1] > 0]\n",
    "        sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "        for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores]:\n",
    "            aadict[phrase] = score\n",
    "            train[name][i] = [aadict]\n",
    "        if i>2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combineList=(train.productList+train.desList+train.searchList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_itf(combineList,'Combine_df_itf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# # Train\n",
    "\n",
    "# In[12]:\n",
    "train=pd.read_csv('~/Desktop/train_clean.csv',encoding='ISO-8859-1',keep_default_na=False,na_values=[])\n",
    "\n",
    "\n",
    "# # TF-IDF\n",
    "\n",
    "# ## Common Vocabulary\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), min_df = 0, stop_words = 'english',encoding='ISO-8859-1')\n",
    "# In[17]:\n",
    "#common\n",
    "combineList=(train.productList+train.desList+train.searchList)\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "combineList_s = pd.Series()\n",
    "for i in range(len(combineList)):\n",
    "    combineList_s=np.append(combineList_s,combineList[i].split())\n",
    "\n",
    "# # Function\n",
    "# In[28]:\n",
    "\n",
    "def df_itf(item,name):\n",
    "    tfidf_matrix=tf.fit_transform(item)   #item should be pd.Series\n",
    "    feature_names = tf.get_feature_names()\n",
    "    dense = tfidf_matrix.todense()\n",
    "    episode=dense.tolist()[0]  ## 这行好像可以去掉， 为啥不是 dense[0]\n",
    "    phrase_scores = [pair for pair in zip(range(0, len(episode)), episode) if pair[1] > 0] #这行也可以去掉了\n",
    "    train[name]=0\n",
    "    for i in range(len(dense)):\n",
    "        aadict = {}\n",
    "        episode=dense[i].tolist()[0]\n",
    "        phrase_scores = [pair for pair in zip(range(0, len(episode)), episode) if pair[1] > 0]\n",
    "        sorted_phrase_scores = sorted(phrase_scores, key=lambda t: t[1] * -1)\n",
    "        for phrase, score in [(feature_names[word_id], score) for (word_id, score) in sorted_phrase_scores]:\n",
    "            #三层loop太多了，要优化下整个算法，现在运算量级是O(n^3)\n",
    "            aadict[phrase] = score \n",
    "            train[name][i] = [aadict]\n",
    "#where is your return??????????????????????????\n",
    "\n",
    "# In[29]:\n",
    "## Common dictionary\n",
    "df_itf(CombineList,'a')\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "train.a[:3]\n",
    "\n",
    "\n",
    "# In[153]:\n",
    "\n",
    "#Concate the dict matrix\n",
    "con_matrix=pd.DataFrame()\n",
    "each_row=pd.DataFrame()\n",
    "def concate_matrix(item):\n",
    "    for i in range(len(item)):\n",
    "        each_row=pd.DataFrame(item[i])\n",
    "        con_matrix = pd.concat([con_matrix,each_row],ignore_index=True)\n",
    "        if i>10:\n",
    "            break\n",
    "\n",
    "\n",
    "# ### SDT\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "# In[100]:\n",
    "\n",
    "# n_components remains unknown\n",
    "\n",
    "svd = TruncatedSVD(n_components = 2, n_iter=15)\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "result = svd.fit(con_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "cosine_similarity([1,1,1],[2,2,3])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
