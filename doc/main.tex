\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}
\ifCLASSINFOpdf
\else
\fi
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Home Depot Product Search Relevance}


\author{Kewen~Zhang,
        Pengfei~Wang,
        Xiaoci~Xing,
        Ziyue~Wu}


\maketitle


\begin{abstract}
In this search relevance project, our goal was to build a model to predict the relevance of search items and product on homedepot.com, given the searching items, resulting product titles and product descriptions. Our team’s solution relies heavily on feature extraction/selection and model ensembling.

Our solution consists of three parts:

1. Text cleaning

Before generating features, we have realized that it’s reasonable to process the data. So we cleaned the data with spelling correction, synonym replacement, removing dots and stop words. Then we selected the optimal solution of N for each N-grams feature.

2. feature extraction

We had tried three types of feature:

a counting features( which is not used in the final result)

b distance features(1~8 grams)

c TF-IDF features (1~8 grams)

3. selection and model ensembling.

Model ensembling consisted of two main steps. Firstly, we trained model library using different models, different parameter settings, and different subsets of the features. Secondly, we generated ensemble submission from the possible ensemble selections. Performance was estimated using cross validation within the training set. We tried both classification and regression to compare our results.

a Neural Net

b General Linear Model

c Machine Learning Methods
\end{abstract}

\IEEEpeerreviewmaketitle



\section{Introduction}
Shoppers rely on Home Depot’s product authority to find and buy the latest products and to get timely solutions to their home improvement needs. From installing a new ceiling fan to remodeling an entire kitchen, with the click of a mouse or tap of the screen, customers expect the correct results to their queries – quickly. Speed, accuracy and delivering a frictionless customer experience are essential.

In this project, we were asked to help them improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results.

Search relevancy is an implicit measure Home Depot uses to gauge how quickly they can get customers to the right products. Currently, human raters evaluate the impact of potential changes to their search algorithms, which is a slow and subjective process. By removing or minimizing human input in search relevance evaluation, Home Depot hopes to increase the number of iterations their team can perform on the current search algorithms.

The relevance is a number between 1 (not relevant) to 3 (highly relevant). For example, a search for "AA battery" would be considered highly relevant to a pack of size AA batteries (relevance = 3), mildly relevant to a cordless drill battery (relevance = 2), and not relevant to a snow shovel (relevance = 1).
\section{Data Cleaning}
\subsection{Word Replacement}
By exploring the provided data, it seems important to perform some word replacements/alignments, e.g., spelling correction and synonym replacement, to align those words with the same or similar meaning.
\subsubsection{Spelling Correction}
We just fixed the typo like "helloWorld" to "hello World", which seperated two individual word. What is more, we delete the stop words like "the", "and", for these words don't lend any support to our training.
\subsubsection{Synonym Replacement}
We replaced the synonym to reduce and simplify our data size.
\subsubsection{Other Replacements}
Including but not limited to removing insignificant punctuation like "\# , .", translating plurality like "feet" to "foot".
\section{Feature Extraction/Selection}
We use function ngram(s, n) to extract string/sentence s’s n-gram (splitted by whitespace), where n = 1, 2, 3.... For example ngram(big red apple, 2) = [big red, red apple].

All the features are extracted for each run (i.e., repeated time) and fold(used in cross-validation and ensembling), and for the entire training and testing set (used in final model building and generating submission).
\subsection{Counting Features}
\subsection{Jaccard Distance}
JaccardCoef(A,B) = $|A\bigcap B|/|A\bigcup B|$
We calculated the distance between "Search Item", "Product Description" and "Product Title" respectively. We thought it was important to compute the distance between "Product Description" and "Product Title" because ...
We tried many "n"s for our results, and found the optimal "n"s, n=3 for Product Title and Search Item, n=2 for Product Title and Product Description, n=8 for Search Item and Product Description.
\subsection{Cosine Similarity with tf-idf}
In the case of the term frequency tf(t,d), the simplest choice is to use the raw frequency of a term in a document. The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents.

For n-gram selection of tf-idf, we have the similar result as Jaccard Coefficient distance.
\subsection{Customized Features}
As most of items have their attributes in data, we choose the most three common attributes among products, brand, color and material. In these features, we used the method the same as Jaccard coefficient. Obviously, as many items only have one word in their color and material description, we would prefer 1-gram here.
\subsection{Combined Features}
Combined all the features above as our predictors.
\section{Regression Analysis}
Because we found the relevance scores in training set are 1, 1.33, 1.5, 1.67, 2, 2.33, 2.5, 2.67. 3, we think regression should be better than classification.
\subsection{Simple Linear Regression}
\begin{itemize}
\item{The basic understanding of relationship}
\item{Try as a starting point}
\end{itemize}
\subsection{Ridge Regression}
\begin{itemize}
\item{Alleciate multicollinearity among predictor variables}
\item{Grid-search to find optimal penalty}
\item{Search Range:} \\
Alpha:     ##10e-6 to 10e-2
\end{itemize}
\subsection{Random Forest}
\begin{itemize}
\item{Trial and error Search Range:}
\item{Grid-search to find optimal parameter} \\
Number of trees in forest: 5-20 \\
Maximum depth of tree: 2-20
\end{itemize}
\subsection{XGBoosting}
\begin{itemize}
\item{Grid-search to find optimal parameter}
\item{Search Range:} \\
Number of tree depth for base learners: 5-20\\
Number 2-20
\end{itemize}
\subsection{Neural Network Regressor}
\begin{itemize}
\item{Layers: 4}
\item{Nodes: 50/layers}
\item{Steps: 5000}
\end{itemize}
\section{Conclusion}
We applied 5-folds cross validation for each regressors to generated the following table:


\ifCLASSOPTIONcaptionsoff
  \newpage
\fi




\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}




\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{picture}}]{John Doe}
\blindtext
\end{IEEEbiography}


\end{document}


